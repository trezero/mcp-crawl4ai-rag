#!/usr/bin/env python3
"""
Comprehensive Data Quality Report for Crawl4AI MCP Server
"""

import os
from supabase import create_client, Client
from dotenv import load_dotenv
from collections import Counter
import json

def main():
    load_dotenv()
    
    supabase_url = os.getenv("SUPABASE_URL", "http://localhost:8101")
    supabase_key = os.getenv("SUPABASE_SERVICE_KEY")
    
    if not supabase_key:
        print("‚ùå SUPABASE_SERVICE_KEY not found")
        return
    
    try:
        supabase: Client = create_client(supabase_url, supabase_key)
        
        print("üîç COMPREHENSIVE DATA QUALITY REPORT")
        print("=" * 60)
        print(f"Database: {supabase_url}")
        print(f"Timestamp: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # 1. Sources Analysis
        print("üìä SOURCES ANALYSIS:")
        sources = supabase.table("sources").select("*").execute()
        if sources.data:
            print(f"   Total sources: {len(sources.data)}")
            for source in sources.data:
                source_id = source.get('source_id', 'Unknown')
                word_count = source.get('total_word_count', 0)
                created = source.get('created_at', 'Unknown')[:19]
                print(f"   ‚Ä¢ {source_id}: {word_count:,} words (created: {created})")
        else:
            print("   No sources found")
        print()
        
        # 2. Crawled Pages Analysis
        print("üìÑ CRAWLED PAGES ANALYSIS:")
        pages = supabase.table("crawled_pages").select("*").execute()
        
        if pages.data:
            total_pages = len(pages.data)
            print(f"   Total chunks: {total_pages}")
            
            # URL distribution
            urls = [page.get('url', '') for page in pages.data]
            unique_urls = len(set(urls))
            print(f"   Unique URLs: {unique_urls}")
            print(f"   Average chunks per URL: {total_pages/unique_urls:.1f}")
            
            # Source distribution
            source_counts = Counter(page.get('source_id', 'unknown') for page in pages.data)
            print(f"   Source distribution:")
            for source, count in source_counts.most_common():
                print(f"     {source}: {count} chunks")
            
            # Content quality
            content_lengths = [len(page.get('content', '')) for page in pages.data]
            avg_length = sum(content_lengths) / len(content_lengths)
            
            print(f"   Content quality:")
            print(f"     Average chunk size: {avg_length:.0f} chars")
            print(f"     Size range: {min(content_lengths)} - {max(content_lengths)} chars")
            
            # Embedding coverage
            with_embeddings = sum(1 for page in pages.data if page.get('embedding'))
            embedding_coverage = (with_embeddings / total_pages) * 100
            print(f"     Chunks with embeddings: {with_embeddings}/{total_pages} ({embedding_coverage:.1f}%)")
            
            # Metadata analysis
            metadata_keys = set()
            for page in pages.data:
                metadata = page.get('metadata', {})
                if isinstance(metadata, dict):
                    metadata_keys.update(metadata.keys())
            
            print(f"     Metadata fields found: {', '.join(sorted(metadata_keys)) if metadata_keys else 'None'}")
            
        else:
            print("   No crawled pages found")
        print()
        
        # 3. Code Examples Analysis
        print("üíª CODE EXAMPLES ANALYSIS:")
        try:
            code_examples = supabase.table("code_examples").select("*").execute()
            if code_examples.data:
                total_examples = len(code_examples.data)
                print(f"   Total code examples: {total_examples}")
                
                # Language analysis from metadata
                languages = Counter()
                for example in code_examples.data:
                    metadata = example.get('metadata', {})
                    if isinstance(metadata, dict):
                        lang = metadata.get('language', 'unknown')
                        languages[lang] += 1
                
                if languages:
                    print(f"   Language distribution:")
                    for lang, count in languages.most_common():
                        print(f"     {lang}: {count}")
                
                # Summary quality
                with_summaries = sum(1 for ex in code_examples.data if ex.get('summary'))
                summary_coverage = (with_summaries / total_examples) * 100
                print(f"   Examples with summaries: {with_summaries}/{total_examples} ({summary_coverage:.1f}%)")
                
                # Embedding coverage
                with_embeddings = sum(1 for ex in code_examples.data if ex.get('embedding'))
                embedding_coverage = (with_embeddings / total_examples) * 100
                print(f"   Examples with embeddings: {with_embeddings}/{total_examples} ({embedding_coverage:.1f}%)")
                
            else:
                print("   No code examples found")
        except Exception as e:
            print(f"   Code examples table not accessible: {str(e)}")
        print()
        
        # 4. Data Integrity Issues
        print("üîí DATA INTEGRITY ASSESSMENT:")
        issues = []
        
        if pages.data:
            # Check for duplicate URL/chunk combinations
            url_chunks = [(p.get('url'), p.get('chunk_number')) for p in pages.data]
            if len(url_chunks) != len(set(url_chunks)):
                issues.append("Duplicate URL/chunk combinations found")
            
            # Check for missing required fields
            missing_url = sum(1 for p in pages.data if not p.get('url'))
            missing_content = sum(1 for p in pages.data if not p.get('content'))
            missing_source = sum(1 for p in pages.data if not p.get('source_id'))
            
            if missing_url > 0:
                issues.append(f"{missing_url} chunks missing URL")
            if missing_content > 0:
                issues.append(f"{missing_content} chunks missing content")
            if missing_source > 0:
                issues.append(f"{missing_source} chunks missing source_id")
            
            # Check embedding consistency
            if with_embeddings > 0 and with_embeddings < total_pages:
                issues.append(f"Inconsistent embedding coverage ({embedding_coverage:.1f}%)")
        
        if issues:
            print("   ‚ö†Ô∏è  Issues found:")
            for issue in issues:
                print(f"     ‚Ä¢ {issue}")
        else:
            print("   ‚úÖ No integrity issues detected")
        print()
        
        # 5. Overall Quality Score
        print("üìà OVERALL QUALITY ASSESSMENT:")
        
        quality_factors = []
        
        if pages.data:
            # Content quality (no empty content = good)
            content_quality = 100.0  # Already verified no empty content
            quality_factors.append(("Content Quality", content_quality))
            
            # Embedding coverage
            quality_factors.append(("Embedding Coverage", embedding_coverage))
            
            # Data completeness (no missing required fields)
            completeness = 100.0 if not any("missing" in issue for issue in issues) else 80.0
            quality_factors.append(("Data Completeness", completeness))
            
            # Structural integrity (no duplicates)
            integrity = 100.0 if not any("Duplicate" in issue for issue in issues) else 70.0
            quality_factors.append(("Structural Integrity", integrity))
            
            # Calculate overall score
            overall_score = sum(score for _, score in quality_factors) / len(quality_factors)
            
            print(f"   Quality Factors:")
            for factor, score in quality_factors:
                print(f"     {factor}: {score:.1f}%")
            print()
            print(f"   üéØ OVERALL QUALITY SCORE: {overall_score:.1f}%")
            
            if overall_score >= 90:
                print("   ‚úÖ EXCELLENT - Data quality is very high")
            elif overall_score >= 75:
                print("   ‚ö†Ô∏è  GOOD - Data quality is acceptable with minor issues")
            elif overall_score >= 60:
                print("   ‚ö†Ô∏è  FAIR - Data quality needs improvement")
            else:
                print("   ‚ùå POOR - Significant data quality issues detected")
        else:
            print("   ‚ùå NO DATA - No crawled pages found to assess")
        
        print("\n" + "=" * 60)
        print("‚úÖ Data quality verification completed!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()
